# ðŸ’¬ Local LLM Chatbot

![Python](https://img.shields.io/badge/Python-3.10%2B-blue?logo=python)
![Streamlit](https://img.shields.io/badge/Streamlit-App-red?logo=streamlit)
![Ollama](https://img.shields.io/badge/Ollama-Local%20LLM-green?logo=ollama)
![License](https://img.shields.io/badge/License-MIT-lightgrey)

> A lightweight, privacy-friendly chatbot powered by **local LLMs** using [Ollama](https://ollama.com/) and [Streamlit](https://streamlit.io/).  
> Runs entirely **offline**, directly on your Mac â€” no API keys, no cost, full control.

---

## ðŸ§  Overview

This project demonstrates how to build a **fully local AI assistant** that uses open-source Large Language Models.  
The chatbot is built with **Python**, **Streamlit**, and **Ollama**, providing a real-time conversational UI similar to ChatGPT â€” but everything runs on your machine.

---

## ðŸš€ Features

- ðŸ§© **Local inference** with `llama3`, `mistral`, or any Ollama-compatible model  
- âš¡ **Instant setup** â€” no API keys, no billing, no internet required  
- ðŸ§  **Conversational memory** within the chat session  
- ðŸ’» **Built with Streamlit** for a clean web interface  
- ðŸ§° **Extensible architecture** â€” easy to integrate new models or APIs later  

---